{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e24e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inner_grid import GridGenerator\n",
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from shapely.geometry import mapping\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import jaydebeapi as jdbc\n",
    "import jpype\n",
    "import os\n",
    "from prepareData import prepare_data\n",
    "from convert import convert_polygon\n",
    "\n",
    "grid = GridGenerator()\n",
    "#!!!!!!!define polygon(s) to prepare in geojson format\n",
    "file = r\"\"\n",
    "path = r\"\"\n",
    "#!!!!!!!define type of data formal/informal/real - infromal from shelter associates are cut and prepared \n",
    "classification = \"formal\"\n",
    "#50m squares all informal settlements.json\n",
    "#100m squares all informal settlements.json\n",
    "#!!!!!!!define cut size 50mx50m or 100mx100m in int format 50/100 (in case of different size, values in next steps (lat,lon) needs to be changed/added)\n",
    "cut_size = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b70b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\JulianaBaumgartnerov\\AppData\\Local\\Temp\\ipykernel_38244\\2380777913.py:2: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  polygons = gpd.read_file(f\"{path}\\{file}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#read the choosen area to cut into squares in geojson format\n",
    "polygons = gpd.read_file(f\"{path}\\{file}\")\n",
    "all_grids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cada220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing polygon 1/1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JulianaBaumgartnerov\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyogrio\\geopandas.py:662: UserWarning: 'crs' was not provided.  The output dataset will not have projection information defined and may not be usable in other systems.\n",
      "  write(\n"
     ]
    }
   ],
   "source": [
    "#cutting into squares - size defined in 1st tile\n",
    "total = len(polygons)\n",
    "\n",
    "for idx, polygon in enumerate(polygons[\"geometry\"]):\n",
    "    progress = f\"{idx + 1}/{total}\"\n",
    "    print(f\"Processing polygon {progress}...\")\n",
    "\n",
    "    if cut_size == 50:\n",
    "        inside_grid = grid.rectangles_inside_polygon(polygon=polygon, size=(0.000451369, 0.00045121))\n",
    "    elif cut_size == 100:\n",
    "        inside_grid = grid.rectangles_inside_polygon(polygon=polygon, size=(0.000902738, 0.00090242))\n",
    "\n",
    "    all_grids.extend(list(inside_grid))\n",
    "\n",
    "gdf = gpd.GeoDataFrame(all_grids, columns=[\"geometry\"])\n",
    "cut_file = gdf.to_file(f'{file}_{classification}_cut_{cut_size}m.json', driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db99414e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\P'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\P'\n",
      "C:\\Users\\JulianaBaumgartnerov\\AppData\\Local\\Temp\\ipykernel_38244\\386773959.py:9: SyntaxWarning: invalid escape sequence '\\P'\n",
      "  os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk-17\\\\bin\\server\"\n",
      "C:\\Users\\JulianaBaumgartnerov\\AppData\\Local\\Temp\\ipykernel_38244\\386773959.py:21: DeprecationWarning: jpype._core.isThreadAttachedToJVM is deprecated, use java.lang.Thread.isAttached instead\n",
      "  if jpype.isJVMStarted() and not jpype.isThreadAttachedToJVM():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1 of 6...\n",
      "Finished processing chunk 1 of 6.\n",
      "\n",
      "Processing chunk 2 of 6...\n",
      "Finished processing chunk 2 of 6.\n",
      "\n",
      "Processing chunk 3 of 6...\n",
      "Finished processing chunk 3 of 6.\n",
      "\n",
      "Processing chunk 4 of 6...\n",
      "Finished processing chunk 4 of 6.\n",
      "\n",
      "Processing chunk 5 of 6...\n",
      "Finished processing chunk 5 of 6.\n",
      "\n",
      "Processing chunk 6 of 6...\n",
      "Finished processing chunk 6 of 6.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!!!!!!change name of table accordingly\n",
    "sql_tablename = \"FEATURES_DB_MAHARASHTRA\"\n",
    "\n",
    "# connect to the IBM DB2 function\n",
    "def connect_to_db():\n",
    "\n",
    "    jar = \"db2jcc4.jar\"\n",
    "    # os.environ['PATH'] = 'C:\\Program Files\\Java\\jdk-22.0.1\\\\bin:' + os.environ['PATH']\n",
    "    os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk-17\\\\bin\\server\"\n",
    "    os.environ[\"CLASSPATH\"] = jar\n",
    "    # print(os.environ)\n",
    "\n",
    "    args = \"-Djava.class.path=%s\" % jar\n",
    "    jvm_path = jpype.getDefaultJVMPath()\n",
    "    try:\n",
    "        jpype.startJVM(jvm_path, args, convertStrings=True)\n",
    "    except Exception as e:\n",
    "        # print('startJVM exception: ', e)\n",
    "        pass\n",
    "\n",
    "    if jpype.isJVMStarted() and not jpype.isThreadAttachedToJVM():\n",
    "        jpype.attachThreadToJVM()\n",
    "        jpype.java.lang.Thread.currentThread().setContextClassLoader(\n",
    "            jpype.java.lang.ClassLoader.getSystemClassLoader()\n",
    "        )\n",
    "\n",
    "    username = \"xxxxxx\"\n",
    "    password = \"xxxxxx\"\n",
    "\n",
    "    drvrpth = \"db2jcc4.jar\"\n",
    "    drvrnm = \"com.ibm.db2.jcc.DB2Driver\"\n",
    "\n",
    "    connstr = \"jdbc:db2://65beb513-5d3d-4101-9001-f42e9dc954b3.brt9d04f0cmqeb8u7740.databases.appdomain.cloud:30371/BLUDB:sslConnection=true;useJDBC4ColumnNameAndLabelSemantics=false;db2.jcc.charsetDecoderEncoder=3;\"\n",
    "\n",
    "    dw_active = \"yes\"\n",
    "    dm_active = \"yes\"\n",
    "\n",
    "    host = \"65beb513-5d3d-4101-9001-f42e9dc954b3.brt9d04f0cmqeb8u7740.databases.appdomain.cloud:30371\"\n",
    "    jdbc_driver_name = \"com.ibm.db2.jcc.DB2Driver\"\n",
    "    #driver needs to be in same folder\n",
    "    jdbc_driver_loc = \"db2jcc4.jar\"\n",
    "\n",
    "    dw_active = \"yes\"\n",
    "    dm_active = \"yes\"\n",
    "\n",
    "    conn = jdbc.connect(drvrnm, connstr, [username, password], drvrpth)\n",
    "\n",
    "    return conn\n",
    "\n",
    "\n",
    "def fetch_builings(cursor, max_lon, min_lon, min_lat, max_lat):\n",
    "    \"\"\"\n",
    "    This particular function is aimed for obtating all entries from defined rectangle for selected SQL table\n",
    "    \"\"\"\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        SELECT\n",
    "            count(ID) as count,\n",
    "            avg(area_in_meters) as avg_area,\n",
    "            max(area_in_meters) as max_area,\n",
    "            avg(height) as avg_height\n",
    "        FROM USER1.{sql_tablename}\n",
    "        WHERE\n",
    "            LONGITUDE <= {max_lon} AND \n",
    "            LONGITUDE >= {min_lon} AND \n",
    "            LATITUDE >= {min_lat} AND \n",
    "            LATITUDE <= {max_lat}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "        data = cursor.fetchall()\n",
    "    except Exception as e:\n",
    "        print(f\"Fetch items error occurred: {e}\")\n",
    "        print(\"Reconnecting to the database try again...\")\n",
    "\n",
    "        conn = connect_to_db()\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(sql)\n",
    "        data = cursor.fetchall()\n",
    "    finally:\n",
    "        # reshape obtained data to the GeoDataFrame\n",
    "        # df = pd.DataFrame(data=data, columns=columns)\n",
    "        # df = gpd.GeoDataFrame(\n",
    "        #     df, geometry=shapely.from_wkt(df.polygon_coordinates.astype(str))\n",
    "        # )\n",
    "        # df = df.drop([\"polygon_coordinates\"], axis=1)\n",
    "        return data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file = (f'{file}_{classification}_cut_{cut_size}m.json')\n",
    "    #gdf_roads = gpd.read_file(\"/real_with_roads.geojson\")  \n",
    "    gdf_bounds = prepare_data(file)\n",
    "    rows = []\n",
    "\n",
    "    conn = connect_to_db()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    chunk_size = 200\n",
    "    rows = []\n",
    "    total_chunks = (len(gdf_bounds) + chunk_size - 1) // chunk_size\n",
    "\n",
    "    for chunk_index, chunk_start in enumerate(range(0, len(gdf_bounds), chunk_size), start=1):\n",
    "        chunk_end = min(chunk_start + chunk_size, len(gdf_bounds))\n",
    "        chunk = gdf_bounds.iloc[chunk_start:chunk_end]\n",
    "\n",
    "        conn = connect_to_db()\n",
    "        cursor = conn.cursor()\n",
    "        print(f\"Processing chunk {chunk_index} of {total_chunks}...\")\n",
    "\n",
    "        for idx, row in chunk.iterrows():\n",
    "            id = row.id\n",
    "            polygon = row.geometry\n",
    "            max_lon = row.max_lon\n",
    "            min_lon = row.min_lon\n",
    "            min_lat = row.min_lat\n",
    "            max_lat = row.max_lat\n",
    "\n",
    "            # Get data from database\n",
    "            data_from_db = fetch_builings(cursor, max_lon, min_lon, min_lat, max_lat)\n",
    "            #parameters from DB\n",
    "            count, avg_area, max_area, avg_height = data_from_db[0]\n",
    "\n",
    "            # roads count \n",
    "            \"\"\"\n",
    "            roads_in_polygon = gdf_roads[gdf_roads.intersects(polygon)]\n",
    "            if len(roads_in_polygon) == 0:\n",
    "                is_road = 0\n",
    "            else:\n",
    "                is_road = 1\n",
    "            \"\"\" \n",
    "\n",
    "            #, res_count, non_res_count\n",
    "\n",
    "            # # Convert polygon to GeoJSON\n",
    "            converted_polygon = convert_polygon(polygon=polygon)\n",
    "\n",
    "            # Create row with parameters from db\n",
    "            db_row = {\n",
    "            \"id\": id,\n",
    "            \"class\": classification,\n",
    "            \"geometry\": converted_polygon,\n",
    "            \"polygon\": polygon,\n",
    "            \"count\": count,\n",
    "            \"avg_area\": avg_area,\n",
    "            # \"median_area\": median_area,\n",
    "            \"max_area\": max_area,\n",
    "            \"avg_height\": avg_height,\n",
    "            #\"res_count\":res_count,\n",
    "           #\"non_res_count\":non_res_count,\n",
    "        }\n",
    "\n",
    "            rows.append(db_row)\n",
    "\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"Finished processing chunk {chunk_index} of {total_chunks}.\\n\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=rows)\n",
    "\n",
    "\n",
    "#df.to_csv(f\"{file}_buildings.csv\")\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbdb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id   class  \\\n",
      "0   72.88946191812047:19.05055549589045  formal   \n",
      "1   72.88946191812047:19.05100670589045  formal   \n",
      "2  72.88946191812047:19.051457915890452  formal   \n",
      "3  72.88946191812047:19.051909125890454  formal   \n",
      "4  72.88946191812047:19.052360335890455  formal   \n",
      "\n",
      "                                            geometry  \\\n",
      "0  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "1  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "2  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "3  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "4  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "\n",
      "                                             polygon  count    avg_area  \\\n",
      "0  POLYGON ((72.88991328712048 19.05055549589045,...      4  122.332600   \n",
      "1  POLYGON ((72.88991328712048 19.05100670589045,...      3  142.202667   \n",
      "2  POLYGON ((72.88991328712048 19.051457915890452...      2  168.976200   \n",
      "3  POLYGON ((72.88991328712048 19.051909125890454...      2  220.275400   \n",
      "4  POLYGON ((72.88991328712048 19.052360335890455...      6   83.739300   \n",
      "\n",
      "   max_area  avg_height  \n",
      "0  208.0059        8.25  \n",
      "1  245.6486       10.50  \n",
      "2  187.8385        9.00  \n",
      "3  242.7886        6.00  \n",
      "4  154.1345        5.50  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "525c64d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "squares = gpd.GeoDataFrame(df, geometry=\"polygon\", crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of rows\n",
    "print(squares[\"id\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "795f9e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = gpd.read_file(\"tiles.geojson\")\n",
    "tiles = tiles.to_crs(squares.crs)\n",
    "\n",
    "# Spatial join: which tile intersects which square\n",
    "join = gpd.sjoin(squares, tiles, how=\"left\", predicate=\"intersects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c41c4",
   "metadata": {},
   "source": [
    "##### Check if there are any duplicates\n",
    "- rows are duplicated in case our 50x50m large square intersect with more tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b663c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of rows: {join[\"id\"].count()}')\n",
    "duplicates_in_id = join[join.duplicated(\"id\", keep=False)]\n",
    "\n",
    "print(f'Number of duplicates in column \"id\": {duplicates_in_id[\"id\"].count()} \\n')\n",
    "print(\"Duplicates in column 'id':\")\n",
    "print(duplicates_in_id[\"id\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d6730b",
   "metadata": {},
   "source": [
    "##### Download required tiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "581f28f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "os.makedirs(\"india_tiles\", exist_ok=True)\n",
    "\n",
    "tile_ids = join[\"tile\"].dropna().unique()\n",
    "\n",
    "for tile_id in tile_ids:\n",
    "    tif_path = f\"india_tiles/{tile_id}.tif\"\n",
    "    tile_key = f\"forests/v1/alsgedi_global_v6_float/chm/{tile_id}.tif\"\n",
    "    if not os.path.exists(tif_path):\n",
    "        try:\n",
    "            s3.download_file(\"dataforgood-fb-data\", tile_key, tif_path)\n",
    "            print(f\"Downloaded {tile_id}.tif\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {tile_id}.tif: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26f2a3",
   "metadata": {},
   "source": [
    "##### Count the number of pixels in the square\n",
    "- currently hardcoded: over 3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cab4c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1156/1156 [00:16<00:00, 70.17it/s]\n"
     ]
    }
   ],
   "source": [
    "heights = []\n",
    "\n",
    "for idx, row in tqdm(join.iterrows(), total=len(join)):\n",
    "    geom = row.polygon\n",
    "    tile_id = row[\"tile\"]\n",
    "    tif_path = f\"india_tiles/{tile_id}.tif\"\n",
    "\n",
    "    try:\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            # Reproject square to raster CRS\n",
    "            square = gpd.GeoSeries([geom], crs=squares.crs).to_crs(src.crs).iloc[0]\n",
    "\n",
    "            # Clip and extract\n",
    "            out_image, _ = mask(src, [mapping(square)], crop=True)\n",
    "            data = out_image[0]\n",
    "            valid = data[data != src.nodata]\n",
    "\n",
    "            # Collect stats\n",
    "            if valid.size > 0:\n",
    "                count_ge_3m = (valid >= 3).sum()\n",
    "\n",
    "                heights.append(\n",
    "                    {\n",
    "                        \"id\": row.name,\n",
    "                        \"tile\": tile_id,\n",
    "                        \"num_pixels_ge_3m\": int(count_ge_3m),\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                heights.append(\n",
    "                    {\n",
    "                        \"id\": row.name,\n",
    "                        \"tile\": tile_id,\n",
    "                        \"num_pixels_ge_3m\": int(count_ge_3m),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with tile {tile_id} and square {row.name}: {e}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_heights = pd.DataFrame(heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa922372",
   "metadata": {},
   "source": [
    "##### Merge duplicated rows together, on \"id\"\n",
    "- \"id\" is row.name (not longitude:latitude)\n",
    "- tile names are concatenated\n",
    "- the pixel counts from multiple rows are added together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a37c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_multiple_tiles = df_heights.groupby(\"id\", as_index=False).agg(\n",
    "    {\"tile\": lambda x: \" | \".join(x.unique()), \"num_pixels_ge_3m\": \"sum\"}\n",
    ")\n",
    "\n",
    "# sum_multiple_tiles.columns\n",
    "# print(sum_multiple_tiles[sum_multiple_tiles[\"id\"] == 14684])\n",
    "# print(f\"count: {sum_multiple_tiles['id'].count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'id' is the index in df_heights\n",
    "sum_multiple_tiles = sum_multiple_tiles.set_index(\"id\")\n",
    "\n",
    "# Join with original GeoDataFrame\n",
    "squares_with_heights = df.join(sum_multiple_tiles, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba66371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "squares_with_heights.to_csv(\n",
    "    r\"formal_cut_50m.json_buildings_trees.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
