{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7375b56f-d724-4e1d-b559-4f7d16a600d9",
   "metadata": {},
   "source": [
    "## This is the data preparation notebook\n",
    "\n",
    "It is used to prepare formal or informal settlement data for model training. The notebook starts by reading the AOI polygon data and dividing it into square grids (e.g., 50×50 m). It then loads building footprint data, filters buildings that intersect each grid tile, and calculates summary statistics i.e building count, average area, maximum area, and average height. Next, it performs a spatial join to identify which forest tiles intersect the grid squares, downloads the corresponding raster data from Meta’s AWS S3 bucket, and for each grid tile clips the raster to its geometry and counts how many pixels have canopy height ≥ 3 m.\n",
    "\n",
    "The notebook works with helper methods exported from three external `.py` files i.e `inner_grid.py`, `prepareData.py`, and `convert.py`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e24e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inner_grid import GridGenerator\n",
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from shapely.geometry import mapping\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import jaydebeapi as jdbc\n",
    "import jpype\n",
    "import os\n",
    "from prepareData import prepare_data\n",
    "from convert import convert_polygon\n",
    "\n",
    "grid = GridGenerator()\n",
    "# define polygon(s) to prepare in geojson format\n",
    "file = r\"input/Informal_Settlements_Nairobi-SpatialCollective-Dissolved.geojson\"\n",
    "\n",
    "# define type of data formal/informal/real\n",
    "classification = \"informal\"\n",
    "\n",
    "# define cut size 50mx50m or 100mx100m in int format 50/100 (in case of different size, values in next steps (lat,lon) needs to be changed/added)\n",
    "cut_size = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "986b70b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AOI polygons successfully loaded with CRS: EPSG:4326\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#read the choosen area polygons to cut into squares in geojson format\n",
    "polygons = gpd.read_file(file)\n",
    "\n",
    "# Check CRS and align if needed\n",
    "if polygons.crs is None:\n",
    "    print(\"Polygons file has no CRS — assigning EPSG:4326...\")\n",
    "    polygons.set_crs(\"EPSG:4326\", inplace=True)\n",
    "elif polygons.crs.to_string() != \"EPSG:4326\":\n",
    "    print(f\"Polygons CRS is {polygons.crs}. Reprojecting to EPSG:4326...\")\n",
    "    polygons = polygons.to_crs(\"EPSG:4326\")\n",
    "print(\"AOI polygons successfully loaded with CRS:\", polygons.crs)\n",
    "    \n",
    "all_grids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cada220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing polygon 1/1...\n",
      "Total number of 50m grid tiles generated: 3130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.12/site-packages/pyogrio/geopandas.py:710: UserWarning: 'crs' was not provided.  The output dataset will not have projection information defined and may not be usable in other systems.\n",
      "  write(\n"
     ]
    }
   ],
   "source": [
    "#cutting into squares - size defined in 1st tile\n",
    "total = len(polygons)\n",
    "\n",
    "for idx, polygon in enumerate(polygons[\"geometry\"]):\n",
    "    progress = f\"{idx + 1}/{total}\"\n",
    "    print(f\"Processing polygon {progress}...\")\n",
    "\n",
    "    if cut_size == 50:\n",
    "        inside_grid = grid.rectangles_inside_polygon(polygon=polygon, size=(0.000451369, 0.00045121))\n",
    "    elif cut_size == 100:\n",
    "        inside_grid = grid.rectangles_inside_polygon(polygon=polygon, size=(0.000902738, 0.00090242))\n",
    "\n",
    "    all_grids.extend(list(inside_grid))\n",
    "\n",
    "gdf = gpd.GeoDataFrame(all_grids, columns=[\"geometry\"])\n",
    "#cut_file = gdf.to_file(f'{file}_{classification}_cut_{cut_size}m.json', driver='GeoJSON')\n",
    "cut_file = gdf.to_file(f'Nairobi_{classification}_{cut_size}m.json', driver='GeoJSON')\n",
    "print(f\"Total number of {cut_size}m grid tiles generated: {len(all_grids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db99414e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building data successfully loaded with CRS: EPSG:4326\n",
      "Processing chunk 1 of 16...\n",
      "Finished processing chunk 1 of 16.\n",
      "\n",
      "Processing chunk 2 of 16...\n",
      "Finished processing chunk 2 of 16.\n",
      "\n",
      "Processing chunk 3 of 16...\n",
      "Finished processing chunk 3 of 16.\n",
      "\n",
      "Processing chunk 4 of 16...\n",
      "Finished processing chunk 4 of 16.\n",
      "\n",
      "Processing chunk 5 of 16...\n",
      "Finished processing chunk 5 of 16.\n",
      "\n",
      "Processing chunk 6 of 16...\n",
      "Finished processing chunk 6 of 16.\n",
      "\n",
      "Processing chunk 7 of 16...\n",
      "Finished processing chunk 7 of 16.\n",
      "\n",
      "Processing chunk 8 of 16...\n",
      "Finished processing chunk 8 of 16.\n",
      "\n",
      "Processing chunk 9 of 16...\n",
      "Finished processing chunk 9 of 16.\n",
      "\n",
      "Processing chunk 10 of 16...\n",
      "Finished processing chunk 10 of 16.\n",
      "\n",
      "Processing chunk 11 of 16...\n",
      "Finished processing chunk 11 of 16.\n",
      "\n",
      "Processing chunk 12 of 16...\n",
      "Finished processing chunk 12 of 16.\n",
      "\n",
      "Processing chunk 13 of 16...\n",
      "Finished processing chunk 13 of 16.\n",
      "\n",
      "Processing chunk 14 of 16...\n",
      "Finished processing chunk 14 of 16.\n",
      "\n",
      "Processing chunk 15 of 16...\n",
      "Finished processing chunk 15 of 16.\n",
      "\n",
      "Processing chunk 16 of 16...\n",
      "Finished processing chunk 16 of 16.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# Read OBI building footprint data from a geojson/parquet file for the target area\n",
    "# Load building footprints from GeoParquet\n",
    "building_data = \"input/Kenya_Nairobi_OBI.geoparquet\"\n",
    "gdf_buildings = pd.read_parquet(building_data)\n",
    "\n",
    "# Decode geometry from WKB (binary)\n",
    "gdf_buildings[\"geometry\"] = gdf_buildings[\"geometry\"].apply(wkb.loads)\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "gdf_buildings = gpd.GeoDataFrame(gdf_buildings, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "\n",
    "# Check CRS and align if needed\n",
    "if gdf_buildings.crs is None:\n",
    "    print(\"Building data file has no CRS — assigning EPSG:4326...\")\n",
    "    gdf_buildings.set_crs(\"EPSG:4326\", inplace=True)\n",
    "elif gdf_buildings.crs.to_string() != \"EPSG:4326\":\n",
    "    print(f\"Building data file CRS is {gdf_buildings.crs}. Reprojecting to EPSG:4326...\")\n",
    "    gdf_buildings = gdf_buildings.to_crs(\"EPSG:4326\")\n",
    "print(\"Building data successfully loaded with CRS:\", gdf_buildings.crs)\n",
    "\n",
    "# Load the square grid tiles (from earlier step)\n",
    "file = f'Nairobi_{classification}_{cut_size}m.json'\n",
    "gdf_bounds = prepare_data(file)  # This returns a GeoDataFrame with tiles and bounding box columns\n",
    "rows = []\n",
    "\n",
    "# Process in chunks for memory efficiency\n",
    "chunk_size = 200\n",
    "total_chunks = (len(gdf_bounds) + chunk_size - 1) // chunk_size\n",
    "\n",
    "for chunk_index, chunk_start in enumerate(range(0, len(gdf_bounds), chunk_size), start=1):\n",
    "    chunk_end = min(chunk_start + chunk_size, len(gdf_bounds))\n",
    "    chunk = gdf_bounds.iloc[chunk_start:chunk_end]\n",
    "\n",
    "    print(f\"Processing chunk {chunk_index} of {total_chunks}...\")\n",
    "\n",
    "    for idx, row in chunk.iterrows():\n",
    "        id = row.id\n",
    "        polygon = row.geometry\n",
    "\n",
    "        # Filter building footprints that intersect this grid tile\n",
    "        buildings_in_tile = gdf_buildings[gdf_buildings.intersects(polygon)] # uses spatial intersection, which internally performs the same bounding-box filtering\n",
    "\n",
    "        # Extract stats from attributes\n",
    "        count = len(buildings_in_tile)\n",
    "        avg_area = buildings_in_tile[\"area_in_meters\"].mean() if count > 0 else 0\n",
    "        max_area = buildings_in_tile[\"area_in_meters\"].max() if count > 0 else 0\n",
    "        avg_height = buildings_in_tile[\"height\"].mean() if count > 0 else 0\n",
    "\n",
    "        # Convert polygon to JSON-ready format\n",
    "        converted_polygon = convert_polygon(polygon=polygon)\n",
    "\n",
    "        # Build output row\n",
    "        db_row = {\n",
    "            \"id\": id,\n",
    "            \"class\": classification,\n",
    "            \"geometry\": converted_polygon,\n",
    "            \"polygon\": polygon,\n",
    "            \"count\": count,\n",
    "            \"avg_area\": avg_area,\n",
    "            \"max_area\": max_area,\n",
    "            \"avg_height\": avg_height,\n",
    "        }\n",
    "\n",
    "        rows.append(db_row)\n",
    "\n",
    "    print(f\"Finished processing chunk {chunk_index} of {total_chunks}.\\n\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data=rows)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(f\"Nairobi_{classification}_{cut_size}m.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6abbdb37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      id     class  \\\n",
      "0  36.691094749475084:-1.286789340392825  informal   \n",
      "1   36.69199748747508:-1.285435710392825  informal   \n",
      "2  36.692448856475075:-1.285435710392825  informal   \n",
      "3   36.69290022547507:-1.285435710392825  informal   \n",
      "4   36.69335159447507:-1.285435710392825  informal   \n",
      "\n",
      "                                            geometry  \\\n",
      "0  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "1  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "2  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "3  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "4  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "\n",
      "                                             polygon  count   avg_area  \\\n",
      "0  POLYGON ((36.69154611847508 -1.286789340392825...     45  44.745702   \n",
      "1  POLYGON ((36.692448856475075 -1.28543571039282...     24  47.710196   \n",
      "2  POLYGON ((36.69290022547507 -1.285435710392825...     29  57.243062   \n",
      "3  POLYGON ((36.69335159447507 -1.285435710392825...     40  43.590285   \n",
      "4  POLYGON ((36.693802963475065 -1.28543571039282...     36  41.198797   \n",
      "\n",
      "   max_area  avg_height  \n",
      "0  251.4552       4.500  \n",
      "1  125.2081       4.625  \n",
      "2  130.3449       4.500  \n",
      "3  130.3449       4.500  \n",
      "4   93.5896       4.500  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "525c64d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "squares = gpd.GeoDataFrame(df, geometry=\"polygon\", crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5f4bf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3130\n"
     ]
    }
   ],
   "source": [
    "# check number of rows\n",
    "print(squares[\"id\"].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35405a4-cfb6-483d-aeaf-adccd4c85232",
   "metadata": {},
   "source": [
    "Reads the forest tiles dowloaded in the first notebook, checks intersection with the square grids then appends attributes from the forest tiles to the squares GeoDataFrame/grid.\n",
    "\n",
    "The file contains vectore metadata index ie polygon extents + Ids of all raster tiles -- used to identify which tiles overlap the AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "795f9e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiles = gpd.read_file(\"tiles.geojson\")\n",
    "tiles = tiles.to_crs(squares.crs)\n",
    "\n",
    "# Spatial join: which tile intersects which square\n",
    "join = gpd.sjoin(squares, tiles, how=\"left\", predicate=\"intersects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c41c4",
   "metadata": {},
   "source": [
    "##### Check if there are any duplicates\n",
    "- rows are duplicated in case our 50x50m large square intersect with more tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b663c6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 3130\n",
      "Number of duplicates in column \"id\": 0 \n",
      "\n",
      "Duplicates in column 'id':\n",
      "Series([], Name: id, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of rows: {join[\"id\"].count()}')\n",
    "duplicates_in_id = join[join.duplicated(\"id\", keep=False)]\n",
    "\n",
    "print(f'Number of duplicates in column \"id\": {duplicates_in_id[\"id\"].count()} \\n')\n",
    "print(\"Duplicates in column 'id':\")\n",
    "print(duplicates_in_id[\"id\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9617a028-2a78-47f3-b09a-8585b35109eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      id     class  \\\n",
      "0  36.691094749475084:-1.286789340392825  informal   \n",
      "1   36.69199748747508:-1.285435710392825  informal   \n",
      "2  36.692448856475075:-1.285435710392825  informal   \n",
      "3   36.69290022547507:-1.285435710392825  informal   \n",
      "4   36.69335159447507:-1.285435710392825  informal   \n",
      "\n",
      "                                            geometry  \\\n",
      "0  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "1  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "2  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "3  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "4  {\"type\": \"Feature\", \"geometry\": {\"type\": \"Poly...   \n",
      "\n",
      "                                             polygon  count   avg_area  \\\n",
      "0  POLYGON ((36.69155 -1.28679, 36.69155 -1.28634...     45  44.745702   \n",
      "1  POLYGON ((36.69245 -1.28544, 36.69245 -1.28498...     24  47.710196   \n",
      "2  POLYGON ((36.6929 -1.28544, 36.6929 -1.28498, ...     29  57.243062   \n",
      "3  POLYGON ((36.69335 -1.28544, 36.69335 -1.28498...     40  43.590285   \n",
      "4  POLYGON ((36.6938 -1.28544, 36.6938 -1.28498, ...     36  41.198797   \n",
      "\n",
      "   max_area  avg_height  index_right       tile  \n",
      "0  251.4552       4.500        38329  300110102  \n",
      "1  125.2081       4.625        38329  300110102  \n",
      "2  130.3449       4.500        38329  300110102  \n",
      "3  130.3449       4.500        38329  300110102  \n",
      "4   93.5896       4.500        38329  300110102  \n"
     ]
    }
   ],
   "source": [
    "print(join.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d6730b",
   "metadata": {},
   "source": [
    "##### Download required tiffs from Meta’s AWS S3 bucket\n",
    "Downloads all raster tiles that overlap the AOI as identfed from the prev cell -- contains the actual raster data i.e canopy height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "581f28f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\", config=Config(signature_version=UNSIGNED))\n",
    "tile_folder = \"Nairobi_tiles\"\n",
    "os.makedirs(tile_folder, exist_ok=True)\n",
    "\n",
    "tile_ids = join[\"tile\"].dropna().unique()\n",
    "\n",
    "for tile_id in tile_ids:\n",
    "    tif_path = f\"{tile_folder}/{tile_id}.tif\"\n",
    "    tile_key = f\"forests/v1/alsgedi_global_v6_float/chm/{tile_id}.tif\"\n",
    "    if not os.path.exists(tif_path):\n",
    "        try:\n",
    "            s3.download_file(\"dataforgood-fb-data\", tile_key, tif_path)\n",
    "            print(f\"Downloaded {tile_id}.tif\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {tile_id}.tif: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26f2a3",
   "metadata": {},
   "source": [
    "##### Count the number of pixels in the square\n",
    "This cell iterates over each grid square (join), opens the corresponding tif tile, clips the raster to that square’s geometry, and counts how many pixels in that area have canopy height >= 3 m.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7cab4c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3130/3130 [00:34<00:00, 91.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extracts raster data (canopy height) from the downloaded .tif files associated with each grid\n",
    "heights = []\n",
    "\n",
    "for idx, row in tqdm(join.iterrows(), total=len(join)):\n",
    "    geom = row.polygon\n",
    "    tile_id = row[\"tile\"]\n",
    "    tif_path = f\"{tile_folder}/{tile_id}.tif\"\n",
    "\n",
    "    try:\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            # Reproject square to raster CRS\n",
    "            square = gpd.GeoSeries([geom], crs=squares.crs).to_crs(src.crs).iloc[0]\n",
    "\n",
    "            # Clip and extract\n",
    "            out_image, _ = mask(src, [mapping(square)], crop=True)\n",
    "            data = out_image[0]\n",
    "            valid = data[data != src.nodata]\n",
    "\n",
    "            # Collect stats\n",
    "            if valid.size > 0:\n",
    "                count_ge_3m = (valid >= 3).sum()\n",
    "\n",
    "                heights.append(\n",
    "                    {\n",
    "                        \"id\": row.name,\n",
    "                        \"tile\": tile_id,\n",
    "                        \"num_pixels_ge_3m\": int(count_ge_3m),\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                heights.append(\n",
    "                    {\n",
    "                        \"id\": row.name,\n",
    "                        \"tile\": tile_id,\n",
    "                        \"num_pixels_ge_3m\": int(count_ge_3m),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with tile {tile_id} and square {row.name}: {e}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_heights = pd.DataFrame(heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa922372",
   "metadata": {},
   "source": [
    "##### Merge duplicated rows together, based on \"id\"\n",
    "- \"id\" is row.name (not longitude:latitude)\n",
    "- tile names are concatenated\n",
    "- the pixel counts from multiple rows are added together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "791a37c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_multiple_tiles = df_heights.groupby(\"id\", as_index=False).agg(\n",
    "    {\"tile\": lambda x: \" | \".join(x.unique()), \"num_pixels_ge_3m\": \"sum\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5dd4aecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'id' is the index in df_heights\n",
    "sum_multiple_tiles = sum_multiple_tiles.set_index(\"id\")\n",
    "\n",
    "# Join with original GeoDataFrame\n",
    "squares_with_heights = df.join(sum_multiple_tiles, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba66371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "squares_with_heights.to_csv(f\"Nairobi_{classification}_{cut_size}m_trees.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af39f9-1004-434a-a7c6-b9cdc9f21e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
